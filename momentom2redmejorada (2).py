# -*- coding: utf-8 -*-
"""MomentoM2RedMejorada

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mpLPl6DzN2grJZgxqdcaUSUO8SiU4sZY
"""

# Importar las librerías necesarias
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.neural_network import MLPClassifier

# Importar el dataset y limpiarlo
df = pd.read_csv("Calorias FastF.csv")
df = df.apply(pd.to_numeric, errors='coerce')
df['Calories'] = np.where(df['Calories'] > 219, 'Si', "No")
df = df.dropna()

# Definir X y Y (Combinación de x e y originales)
X = df[['Calories from\nFat', 'Saturated Fat\n(g)']].values
Y = df['Calories'].values

# Dividir el dataset en conjuntos de entrenamiento y prueba (80% entrenamiento, 20% prueba)
x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Dividir el conjunto de entrenamiento en entrenamiento y validación (80% entrenamiento, 20% validación)
x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)

# Gráfica para mostrar la separación de los conjuntos (entrenamiento, validación y prueba)
tamaños = [len(y_train), len(y_val), len(y_test)]
etiquetas = ['Entrenamiento', 'Validación', 'Prueba']

plt.figure(figsize=(8, 6))
plt.bar(etiquetas, tamaños, color=['skyblue', 'orange', 'green'])
plt.title("Tamaño de los conjuntos de entrenamiento, validación y prueba")
plt.xlabel("Conjunto")
plt.ylabel("Número de muestras")
plt.show()

# Inicialización y entrenamiento del modelo de red neuronal (sin ajustar)
modelo = MLPClassifier(hidden_layer_sizes=(10,), max_iter=1000, random_state=42)
modelo.fit(x_train, y_train)

# Generar las predicciones para el conjunto de entrenamiento
predicciones_train = modelo.predict(x_train)

# Generar la matriz de confusión para los datos de entrenamiento
cm_train = confusion_matrix(y_train, predicciones_train)

# Crear la visualización de la matriz de confusión para entrenamiento
disp_train = ConfusionMatrixDisplay(confusion_matrix=cm_train, display_labels=modelo.classes_)

# Mostrar visualmente la matriz de confusión para los datos de entrenamiento
disp_train.plot(cmap=plt.cm.Reds)
plt.title("Entrenamiento")
plt.show()

# Calcular las métricas de evaluación en el conjunto de entrenamiento
precision = precision_score(y_train, predicciones_train, average='weighted')
recall = recall_score(y_train, predicciones_train, average='weighted')
f1 = f1_score(y_train, predicciones_train, average='weighted')

# Mostrar las métricas de evaluación
print(f"Precisión en entrenamiento: {precision:.2f}")
print(f"Recall en entrenamiento: {recall:.2f}")
print(f"F1 Score en entrenamiento: {f1:.2f} \n")

# RESULTADOS ORIGINALES
# Predicciones en el conjunto de validación
predicciones_val = modelo.predict(x_val)
# Generar la matriz de confusión para los datos de validación
cm_val = confusion_matrix(y_val, predicciones_val)
disp_val = ConfusionMatrixDisplay(confusion_matrix=cm_val, display_labels=modelo.classes_)
disp_val.plot(cmap=plt.cm.Reds)
plt.title("Validación Original")
plt.show()

# Calcular las métricas de evaluación en el conjunto de validación
precision_val = precision_score(y_val, predicciones_val, average='weighted')
recall_val = recall_score(y_val, predicciones_val, average='weighted')
f1_val = f1_score(y_val, predicciones_val, average='weighted')
print(f"Precisión en validación: {precision_val:.2f}")
print(f"Recall en validación: {recall_val:.2f}")
print(f"F1 Score en validación: {f1_val:.2f} \n")

# Predicciones en el conjunto de prueba
predicciones_test = modelo.predict(x_test)
# Generar la matriz de confusión para los datos de prueba
cm_test = confusion_matrix(y_test, predicciones_test)
disp_test = ConfusionMatrixDisplay(confusion_matrix=cm_test, display_labels=modelo.classes_)
disp_test.plot(cmap=plt.cm.Reds)
plt.title("Prueba Original")
plt.show()

# Calcular las métricas de evaluación en el conjunto de prueba
precision_test = precision_score(y_test, predicciones_test, average='weighted')
recall_test = recall_score(y_test, predicciones_test, average='weighted')
f1_test = f1_score(y_test, predicciones_test, average='weighted')
print(f"Precisión en prueba: {precision_test:.2f}")
print(f"Recall en prueba: {recall_test:.2f}")
print(f"F1 Score en prueba: {f1_test:.2f} \n")

# MEJORAS CON TRES TÉCNICAS: GridSearchCV para ajuste de 'alpha', 'hidden_layer_sizes' y 'learning_rate_init'
param_grid = {
    'alpha': [0.0001, 0.001, 0.01],
    'hidden_layer_sizes': [(10,), (50,), (100,)],
    'learning_rate_init': [0.001, 0.01, 0.1]
}

# Buscar los mejores parámetros
grid_search = GridSearchCV(MLPClassifier(max_iter=1000, random_state=42), param_grid, cv=5, scoring='f1_weighted')
grid_search.fit(x_train, y_train)

# Imprimir los mejores parámetros encontrados
print("Mejores parámetros encontrados:", grid_search.best_params_)

# Justificación de la selección de hiperparámetros
# - learning_rate_init: Se seleccionó el valor óptimo a través de GridSearchCV entre [0.001, 0.01, 0.1]. El valor final
#   permite un aprendizaje eficiente sin provocar oscilaciones o estancamiento en el proceso de convergencia.
# - hidden_layer_sizes: Se evaluaron redes con 10, 50 y 100 neuronas en la capa oculta. El mejor rendimiento se
#   observó con 50 neuronas, logrando un buen balance entre capacidad de modelado y prevención de sobreajuste.
# - alpha: Se ajustó la regularización L2 con valores [0.0001, 0.001, 0.01]. El valor final seleccionado ayudó a
#   evitar el sobreajuste penalizando grandes pesos, contribuyendo así a una mejor generalización.

# Entrenar el modelo con los mejores parámetros
best_model = grid_search.best_estimator_

# RESULTADOS DEL MODELO AJUSTADO
# Predicciones en el conjunto de validación ajustado
predicciones_val_ajustado = best_model.predict(x_val)

# Generar la matriz de confusión para los datos de validación ajustado
cm_val_ajustado = confusion_matrix(y_val, predicciones_val_ajustado)
disp_val_ajustado = ConfusionMatrixDisplay(confusion_matrix=cm_val_ajustado, display_labels=best_model.classes_)
disp_val_ajustado.plot(cmap=plt.cm.Reds)
plt.title("Validación con Modelo Ajustado")
plt.show()

# Calcular las métricas de evaluación en el conjunto de validación ajustado
precision_ajustado = precision_score(y_val, predicciones_val_ajustado, average='weighted')
recall_ajustado = recall_score(y_val, predicciones_val_ajustado, average='weighted')
f1_ajustado = f1_score(y_val, predicciones_val_ajustado, average='weighted')
print(f"Precisión ajustada en validación: {precision_ajustado:.2f}")
print(f"Recall ajustado en validación: {recall_ajustado:.2f}")
print(f"F1 Score ajustado en validación: {f1_ajustado:.2f} \n")

# Predicciones en el conjunto de prueba ajustado
predicciones_test_ajustado = best_model.predict(x_test)

# Generar la matriz de confusión para los datos de prueba ajustado
cm_test_ajustado = confusion_matrix(y_test, predicciones_test_ajustado)
disp_test_ajustado = ConfusionMatrixDisplay(confusion_matrix=cm_test_ajustado, display_labels=best_model.classes_)
disp_test_ajustado.plot(cmap=plt.cm.Reds)
plt.title("Prueba con Modelo Ajustado")
plt.show()

# Calcular las métricas de evaluación en el conjunto de prueba ajustado
precision_test_ajustado = precision_score(y_test, predicciones_test_ajustado, average='weighted')
recall_test_ajustado = recall_score(y_test, predicciones_test_ajustado, average='weighted')
f1_test_ajustado = f1_score(y_test, predicciones_test_ajustado, average='weighted')
print(f"Precisión ajustada en prueba: {precision_test_ajustado:.2f}")
print(f"Recall ajustado en prueba: {recall_test_ajustado:.2f}")
print(f"F1 Score ajustado en prueba: {f1_test_ajustado:.2f} \n")

# COMPARATIVA DE MÉTRICAS ANTES Y DESPUÉS DE LOS AJUSTES
etiquetas_conjuntos = ['Original Validación', 'Ajustado Validación', 'Original Prueba', 'Ajustado Prueba']
precision_original = [precision_val, precision_ajustado, precision_test, precision_test_ajustado]
recall_original = [recall_val, recall_ajustado, recall_test, recall_test_ajustado]
f1_original = [f1_val, f1_ajustado, f1_test, f1_test_ajustado]

# Gráfica comparativa
plt.figure(figsize=(10, 6))
plt.bar(etiquetas_conjuntos[:2], [precision_original[0], precision_original[1]], color=['blue', 'orange'], label="Precisión")
plt.bar(etiquetas_conjuntos[2:], [precision_original[2], precision_original[3]], color=['green', 'red'], label="Recall")
plt.title("Comparación de Precisión y Recall antes y después de los ajustes")
plt.ylabel("Métricas")
plt.legend()
plt.show()

# Mostrar comparativa de F1 Score
plt.figure(figsize=(10, 6))
plt.bar(['F1 Original Validación', 'F1 Ajustado Validación', 'F1 Original Prueba', 'F1 Ajustado Prueba'],
        [f1_original[0], f1_original[1], f1_original[2], f1_original[3]], color=['purple', 'orange', 'green', 'red'])
plt.title("Comparación de F1 Score antes y después de los ajustes")
plt.ylabel("F1 Score")
plt.show()

# Gráfica comparativa del desempeño entre entrenamiento, validación y prueba
etiquetas_conjuntos = ['Entrenamiento', 'Validación', 'Prueba']
precision_original = [precision_val, precision_test, precision_test_ajustado]
recall_original = [recall_val, recall_test, recall_test_ajustado]
f1_original = [f1_val, f1_test, f1_test_ajustado]

plt.figure(figsize=(10, 6))
plt.plot(etiquetas_conjuntos, precision_original, marker='o', label='Precisión', color='blue')
plt.plot(etiquetas_conjuntos, recall_original, marker='o', label='Recall', color='orange')
plt.plot(etiquetas_conjuntos, f1_original, marker='o', label='F1 Score', color='green')
plt.title("Comparación del rendimiento entre entrenamiento, validación y prueba")
plt.xlabel("Conjunto de datos")
plt.ylabel("Métricas")
plt.legend()
plt.grid(True)
plt.show()